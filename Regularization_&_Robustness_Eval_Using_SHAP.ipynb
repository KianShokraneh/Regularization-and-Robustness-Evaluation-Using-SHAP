{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUBc0rVDIJhsz3YhgcUUsY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KianShokraneh/Regularization-and-Robustness-Evaluation-Using-SHAP/blob/main/Regularization_%26_Robustness_Eval_Using_SHAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BcEJktzcwq2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision shap captum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4wdRozHyY5pj",
        "outputId": "7bf5c281-c0f9-47d5-90fc-87232c00a7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Collecting shap\n",
            "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting captum\n",
            "  Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.4)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.6.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=d0dc070e99f862bd5dfb2061794991f78a7ac4f26ab633f1e7fcf573af67775f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built lime\n",
            "Installing collected packages: slicer, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, shap, nvidia-cusolver-cu12, lime, captum\n",
            "Successfully installed captum-0.7.0 lime-0.2.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 shap-0.46.0 slicer-0.0.8\n",
            "Collecting git+https://github.com/AnotherSamWilson/fastshap.git\n",
            "  Cloning https://github.com/AnotherSamWilson/fastshap.git to /tmp/pip-req-build-tbbbo9r8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AnotherSamWilson/fastshap.git /tmp/pip-req-build-tbbbo9r8\n",
            "  Resolved https://github.com/AnotherSamWilson/fastshap.git to commit 2a791a321909143da5606fb3cb092434d413850d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastshap==0.2.1) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastshap==0.2.1) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fastshap==0.2.1) (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->fastshap==0.2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastshap==0.2.1) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastshap==0.2.1) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastshap==0.2.1) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastshap==0.2.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastshap==0.2.1) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->fastshap==0.2.1) (1.16.0)\n",
            "Building wheels for collected packages: fastshap\n",
            "  Building wheel for fastshap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastshap: filename=fastshap-0.2.1-py3-none-any.whl size=20589 sha256=4dd82fc355d669162f410062963232fc5cbffac2d87460b268bbaf9d302016f2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-weq02szr/wheels/d4/ad/84/a24de5b36c40b577989c2fb48c2b6378d0bebf32747f5a3c98\n",
            "Successfully built fastshap\n",
            "Installing collected packages: fastshap\n",
            "Successfully installed fastshap-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from captum.attr import DeepLift, GradientShap\n",
        "import shap\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "vRUp-U4RJgoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 3\n",
        "epsilon = 0.1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "3d6Rk2f0Jke9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(model, loss_fn, images, labels, epsilon):\n",
        "    images.requires_grad = True\n",
        "    outputs = model(images)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    grad = images.grad.data\n",
        "    adv_images = images + epsilon * grad.sign()\n",
        "    adv_images = torch.clamp(adv_images, 0, 1)\n",
        "    return adv_images\n",
        "\n",
        "def shap_reg_loss(model, images, adv_images, labels):\n",
        "    explainer = shap.DeepExplainer(model, images[:10].to(device))\n",
        "    shap_values = explainer.shap_values(images[:10].to(device))\n",
        "    shap_values_adv = explainer.shap_values(adv_images[:10].to(device))\n",
        "    loss = 0\n",
        "    for i in range(len(shap_values)):\n",
        "        loss += torch.mean((torch.tensor(shap_values[i]) - torch.tensor(shap_values_adv[i])) ** 2)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def deeplift_reg_loss(model, images, adv_images, labels):\n",
        "    deeplift = DeepLift(model)\n",
        "    baseline = torch.zeros_like(images).to(device)\n",
        "    shap_values = deeplift.attribute(images, baselines=baseline, target=labels)\n",
        "    shap_values_adv = deeplift.attribute(adv_images, baselines=baseline, target=labels)\n",
        "    shap_values, shap_values_adv = torch.tensor(shap_values, dtype=torch.float32).to(device), torch.tensor(shap_values_adv, dtype=torch.float32).to(device)\n",
        "    loss = torch.mean((shap_values - shap_values_adv) ** 2)\n",
        "    return loss\n",
        "\n",
        "def gradientshap_reg_loss(model, images, adv_images, labels):\n",
        "    gs = GradientShap(model)\n",
        "    baseline_dist = torch.randn((20, *images.shape[1:]), requires_grad=True).to(device)\n",
        "    shap_values = gs.attribute(images, baselines=baseline_dist, target=labels)\n",
        "    shap_values_adv = gs.attribute(adv_images, baselines=baseline_dist, target=labels)\n",
        "    shap_values, shap_values_adv = torch.tensor(shap_values, dtype=torch.float32).to(device), torch.tensor(shap_values_adv, dtype=torch.float32).to(device)\n",
        "    loss = torch.mean((shap_values - shap_values_adv) ** 2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "r9sVDkGvFal8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_shap(model, train_loader, criterion, optimizer, epsilon, device, reg_loss_fn, num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        i=0\n",
        "        for images, labels in train_loader:\n",
        "            if i%100==0:\n",
        "              print(i)\n",
        "            i+=1\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            adv_images = fgsm_attack(model, criterion, images, labels, epsilon)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            shap_loss = reg_loss_fn(model, images, adv_images, labels)\n",
        "\n",
        "            total_loss = loss + shap_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} completed')\n",
        "\n",
        "def train_without_shap(model, train_loader, criterion, optimizer, device, num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        i=0\n",
        "        for images, labels in train_loader:\n",
        "            if i%100==0:\n",
        "              print(i)\n",
        "            i+=1\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} completed')\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "HnhKnIGIj65M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ce = SimpleModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ce = optim.Adam(model_ce.parameters(), lr=learning_rate)\n",
        "print(\"Training with CE loss only\")\n",
        "train_without_shap(model_ce, train_loader, criterion, optimizer_ce, device, num_epochs)\n",
        "accuracy_ce = evaluate(model_ce, test_loader, device)\n",
        "print(f'Accuracy with CE loss only: {accuracy_ce:.2f}%')\n",
        "\n",
        "model_shap = SimpleModel().to(device)\n",
        "optimizer_shap = optim.Adam(model_shap.parameters(), lr=learning_rate)\n",
        "print(\"Training with original SHAP regularization\")\n",
        "train_with_shap(model_shap, train_loader, criterion, optimizer_shap, epsilon, device, shap_reg_loss, num_epochs)\n",
        "accuracy_shap = evaluate(model_shap, test_loader, device)\n",
        "print(f'Accuracy with original SHAP regularization: {accuracy_shap:.2f}%')\n",
        "\n",
        "model_dl = SimpleModel().to(device)\n",
        "optimizer_dl = optim.Adam(model_dl.parameters(), lr=learning_rate)\n",
        "print(\"Training with DeepLIFT regularization\")\n",
        "train_with_shap(model_dl, train_loader, criterion, optimizer_dl, epsilon, device, deeplift_reg_loss, num_epochs)\n",
        "accuracy_dl = evaluate(model_dl, test_loader, device)\n",
        "print(f'Accuracy with DeepLIFT regularization: {accuracy_dl:.2f}%')\n",
        "\n",
        "model_gs = SimpleModel().to(device)\n",
        "optimizer_gs = optim.Adam(model_gs.parameters(), lr=learning_rate)\n",
        "print(\"Training with GradientSHAP regularization\")\n",
        "train_with_shap(model_gs, train_loader, criterion, optimizer_gs, epsilon, device, gradientshap_reg_loss, num_epochs)\n",
        "accuracy_gs = evaluate(model_gs, test_loader, device)\n",
        "print(f'Accuracy with GradientSHAP regularization: {accuracy_gs:.2f}%')\n",
        "\n",
        "print(f'Accuracy with CE loss only: {accuracy_ce:.2f}%')\n",
        "print(f'Accuracy with original SHAP regularization: {accuracy_shap:.2f}%')\n",
        "print(f'Accuracy with DeepLIFT regularization: {accuracy_dl:.2f}%')\n",
        "print(f'Accuracy with GradientSHAP regularization: {accuracy_gs:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hZcORrHJqd8",
        "outputId": "469662c0-94d9-488f-daac-0bea8ba76849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with CE loss only\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 1/3 completed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 2/3 completed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 3/3 completed\n",
            "Accuracy with CE loss only: 96.97%\n",
            "Training with original SHAP regularization\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 1/3 completed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 2/3 completed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 3/3 completed\n",
            "Accuracy with original SHAP regularization: 96.92%\n",
            "Training with DeepLIFT regularization\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/captum/attr/_core/deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
            "               activations. The hooks and attributes will be removed\n",
            "            after the attribution is finished\n",
            "  warnings.warn(\n",
            "<ipython-input-42-823f6f2c0049>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  shap_values, shap_values_adv = torch.tensor(shap_values, dtype=torch.float32).to(device), torch.tensor(shap_values_adv, dtype=torch.float32).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 1/3 completed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 2/3 completed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 3/3 completed\n",
            "Accuracy with DeepLIFT regularization: 96.85%\n",
            "Training with GradientSHAP regularization\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-823f6f2c0049>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  shap_values, shap_values_adv = torch.tensor(shap_values, dtype=torch.float32).to(device), torch.tensor(shap_values_adv, dtype=torch.float32).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 1/3 completed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 2/3 completed\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "Epoch 3/3 completed\n",
            "Accuracy with GradientSHAP regularization: 96.95%\n",
            "Accuracy with CE loss only: 96.97%\n",
            "Accuracy with original SHAP regularization: 96.92%\n",
            "Accuracy with DeepLIFT regularization: 96.85%\n",
            "Accuracy with GradientSHAP regularization: 96.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_ce.state_dict(), 'model_ce.pth')\n",
        "torch.save(model_shap.state_dict(), 'model_shap.pth')\n",
        "torch.save(model_dl.state_dict(), 'model_dl.pth')\n",
        "torch.save(model_gs.state_dict(), 'model_gs.pth')"
      ],
      "metadata": {
        "id": "PD2lsO5XhnkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ce = SimpleModel().to(device)\n",
        "model_shap = SimpleModel().to(device)\n",
        "model_dl = SimpleModel().to(device)\n",
        "model_gs = SimpleModel().to(device)\n",
        "\n",
        "model_ce.load_state_dict(torch.load('model_ce.pth', map_location=torch.device(device)))\n",
        "model_shap.load_state_dict(torch.load('model_shap.pth',map_location=torch.device(device)))\n",
        "model_dl.load_state_dict(torch.load('model_dl.pth', map_location=torch.device(device)))\n",
        "model_gs.load_state_dict(torch.load('model_gs.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRF-XlhNeHWH",
        "outputId": "4861ea20-f4d7-4d20-da88-18a9b6a70a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_ce = evaluate(model_ce, test_loader, device)\n",
        "accuracy_shap = evaluate(model_shap, test_loader, device)\n",
        "accuracy_dl = evaluate(model_dl, test_loader, device)\n",
        "accuracy_gs = evaluate(model_gs, test_loader, device)\n",
        "\n",
        "print(f'Accuracy with CE loss only: {accuracy_ce:.2f}%')\n",
        "print(f'Accuracy with original SHAP regularization: {accuracy_shap:.2f}%')\n",
        "print(f'Accuracy with DeepLIFT regularization: {accuracy_dl:.2f}%')\n",
        "print(f'Accuracy with GradientSHAP regularization: {accuracy_gs:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeh9aUecVEwL",
        "outputId": "33d725fb-3128-47f4-c3a4-5fc53f1d5d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with CE loss only: 96.97%\n",
            "Accuracy with original SHAP regularization: 96.92%\n",
            "Accuracy with DeepLIFT regularization: 96.85%\n",
            "Accuracy with GradientSHAP regularization: 96.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_shap_values(model, images, device):\n",
        "    model.eval()\n",
        "    explainer = shap.DeepExplainer(model, images.to(device))\n",
        "    shap_values = explainer.shap_values(images.to(device))\n",
        "    return shap_values\n",
        "\n",
        "def evaluate_robustness(model, test_loader, epsilon, device):\n",
        "    model.eval()\n",
        "    shap_corrs = []\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images[:10].to(device), labels[:10].to(device)\n",
        "\n",
        "        adv_images = fgsm_attack(model, nn.CrossEntropyLoss(), images, labels, epsilon)\n",
        "\n",
        "        shap_values_clean = compute_shap_values(model, images, device)\n",
        "        shap_values_adv = compute_shap_values(model, adv_images, device)\n",
        "\n",
        "        for i in range(len(shap_values_clean)):\n",
        "            for j in range(len(shap_values_clean[i])):\n",
        "                clean_vals = shap_values_clean[i][j].flatten()\n",
        "                adv_vals = shap_values_adv[i][j].flatten()\n",
        "                if len(clean_vals) > 1 and len(adv_vals) > 1:\n",
        "                    corr, _ = pearsonr(clean_vals, adv_vals)\n",
        "                    shap_corrs.append(corr)\n",
        "\n",
        "    mean_corr = np.mean(shap_corrs)\n",
        "    return mean_corr*100\n",
        "\n",
        "\n",
        "robustness_ce = evaluate_robustness(model_ce, test_loader, epsilon, device)\n",
        "print(f'Robustness with CE loss only: {robustness_ce:.2f}%')\n",
        "\n",
        "robustness_shap = evaluate_robustness(model_shap, test_loader, epsilon, device)\n",
        "print(f'Robustness with original SHAP regularization: {robustness_shap:.2f}%')\n",
        "\n",
        "robustness_dl = evaluate_robustness(model_dl, test_loader, epsilon, device)\n",
        "print(f'Robustness with DeepLIFT regularization: {robustness_dl:.2f}%')\n",
        "\n",
        "robustness_gs = evaluate_robustness(model_gs, test_loader, epsilon, device)\n",
        "print(f'Robustness with GradientSHAP regularization: {robustness_gs:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb-PwtI3VevR",
        "outputId": "47070c51-1dfc-4cfc-9766-d9f9175d1a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robustness with CE loss only: 94.25%\n",
            "Robustness with original SHAP regularization: 94.44%\n",
            "Robustness with DeepLIFT regularization: 94.36%\n",
            "Robustness with GradientSHAP regularization: 94.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradshap_values(model, images, labels, device):\n",
        "    model.eval()\n",
        "    gs = GradientShap(model)\n",
        "    baseline_dist = torch.randn((20, *images.shape[1:]), requires_grad=True).to(device)\n",
        "    shap_values = gs.attribute(images.to(device), baselines=baseline_dist, target=labels)\n",
        "    return shap_values\n",
        "\n",
        "def evaluate_robustness_gradshap(model, test_loader, epsilon, device, num_samples=10):\n",
        "    model.eval()\n",
        "    shap_corrs = []\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        adv_images = fgsm_attack(model, nn.CrossEntropyLoss(), images, labels, epsilon)\n",
        "\n",
        "        shap_values_clean = compute_gradshap_values(model, images, labels, device)\n",
        "        shap_values_adv = compute_gradshap_values(model, adv_images, labels, device)\n",
        "\n",
        "        for i in range(len(shap_values_clean)):\n",
        "            for j in range(len(shap_values_clean[i])):\n",
        "                clean_vals = shap_values_clean[i][j].flatten()\n",
        "                adv_vals = shap_values_adv[i][j].flatten()\n",
        "                if len(clean_vals) > 1 and len(adv_vals) > 1:\n",
        "                    corr, _ = pearsonr(clean_vals.cpu().numpy(), adv_vals.cpu().numpy())\n",
        "                    shap_corrs.append(corr)\n",
        "\n",
        "    mean_corr = np.mean(shap_corrs)\n",
        "    return mean_corr\n",
        "\n",
        "\n",
        "\n",
        "robustness_ce = evaluate_robustness_gradshap(model_ce, test_loader, epsilon, device)\n",
        "print(f'Robustness with CE loss only: {robustness_ce*100:.4f}%')\n",
        "\n",
        "robustness_shap = evaluate_robustness_gradshap(model_shap, test_loader, epsilon, device)\n",
        "print(f'Robustness with original SHAP regularization: {robustness_shap*100:.4f}%')\n",
        "\n",
        "robustness_dl = evaluate_robustness_gradshap(model_dl, test_loader, epsilon, device)\n",
        "print(f'Robustness with DeepLIFT regularization: {robustness_dl*100:.4f}%')\n",
        "\n",
        "robustness_gs = evaluate_robustness_gradshap(model_gs, test_loader, epsilon, device)\n",
        "print(f'Robustness with GradientSHAP regularization: {robustness_gs*100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iKXhlyBcsOD",
        "outputId": "7c1f0e4d-bc5b-4d52-dbc6-32bde17a128a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robustness with CE loss only: 21.7339%\n",
            "Robustness with original SHAP regularization: 21.3795%\n",
            "Robustness with DeepLIFT regularization: 21.9530%\n",
            "Robustness with GradientSHAP regularization: 21.8906%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_adversarial_accuracy(model, test_loader, epsilon, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        adv_images = fgsm_attack(model, nn.CrossEntropyLoss(), images, labels, epsilon)\n",
        "        outputs = model(adv_images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    adv_accuracy = 100 * correct / total\n",
        "    return adv_accuracy\n",
        "\n",
        "adv_accuracy_ce = evaluate_adversarial_accuracy(model_ce, test_loader, epsilon, device)\n",
        "print(f'Adversarial accuracy with CE loss only: {adv_accuracy_ce:.2f}%')\n",
        "\n",
        "adv_accuracy_shap = evaluate_adversarial_accuracy(model_shap, test_loader, epsilon, device)\n",
        "print(f'Adversarial accuracy with original SHAP regularization: {adv_accuracy_shap:.2f}%')\n",
        "\n",
        "adv_accuracy_dl = evaluate_adversarial_accuracy(model_dl, test_loader, epsilon, device)\n",
        "print(f'Adversarial accuracy with DeepLIFT regularization: {adv_accuracy_dl:.2f}%')\n",
        "\n",
        "adv_accuracy_gs = evaluate_adversarial_accuracy(model_gs, test_loader, epsilon, device)\n",
        "print(f'Adversarial accuracy with GradientSHAP regularization: {adv_accuracy_gs:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNM1FBl9hflR",
        "outputId": "a6d13ad4-5c20-4b5e-ef8b-11baef43bc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial accuracy with CE loss only: 13.99%\n",
            "Adversarial accuracy with original SHAP regularization: 14.35%\n",
            "Adversarial accuracy with DeepLIFT regularization: 12.29%\n",
            "Adversarial accuracy with GradientSHAP regularization: 12.35%\n"
          ]
        }
      ]
    }
  ]
}